{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "\n",
    "\n",
    "url = 'https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt'\n",
    "r = requests.get(url=url)\n",
    "\n",
    "text = r.content.decode()[10454:]\n",
    "text = re.sub(r'<<[^>]*>>', '', text)\n",
    "text = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = [' '.join(text[i:i+n]) for i in range(0,len(text),n)]\n",
    "\n",
    "\n",
    "shake_text = [line for line in text if re.search(r'^\\s{2}', line)]\n",
    "for i in range(0,len(shake_text)):\n",
    "    line = shake_text[i].strip('\\n')\n",
    "    if(re.search(r'^\\s{2}[A-Z\\s]{2,}\\.', line)):\n",
    "        index = line.find('.')\n",
    "        shake_text[i] = line[index+1:]\n",
    "        \n",
    "\n",
    "shake_text = ' '.join(shake_text).split(' ')\n",
    "shake_text = [word.lower() for word in shake_text if word != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33483\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "data_labels = []\n",
    "for i in range(0, int(len(shake_text)*.8),20):\n",
    "    data.append(' '.join(shake_text[i:i+20]))\n",
    "    data_labels.append(1)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shake_files = []\n",
    "non_shake_files.append(open('data/non1', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non2', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non3', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non4', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non5', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non6', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non7', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non8', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non9', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non10', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non11', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non12', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non13', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non14', 'r').read().split(' '))\n",
    "non_shake_files.append(open('data/non15', 'r').read().split(' '))\n",
    "\n",
    "for file in non_shake_files:\n",
    "    for i in range(0, len(file), 20):\n",
    "        quote = ' '.join(file[i:i+20])\n",
    "        new_quote = quote.replace('\\n', ' ').lower()\n",
    "        data.append(new_quote)\n",
    "        data_labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63542\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "  \n",
    "vocab_size = 7500\n",
    "tokenize = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size) \n",
    "tokenize.fit_on_texts(data)\n",
    "\n",
    "import pickle\n",
    "with open('tokenize.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "data = tokenize.texts_to_sequences(data)\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, data_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=20)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44479, 20)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[10000:]\n",
    "x_validation = x_train[:10000]\n",
    "\n",
    "y_val = y_train[10000:]\n",
    "y_validation = y_train[:10000]\n",
    "\n",
    "print(len(x_val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34479 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 1s 34us/step - loss: 0.6822 - acc: 0.5557 - val_loss: 0.6615 - val_acc: 0.6850\n",
      "\n",
      "Epoch 2/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 10us/step - loss: 0.6144 - acc: 0.7644 - val_loss: 0.5517 - val_acc: 0.8137\n",
      "\n",
      "Epoch 3/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 10us/step - loss: 0.4687 - acc: 0.8400 - val_loss: 0.4018 - val_acc: 0.8520\n",
      "\n",
      "Epoch 4/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 10us/step - loss: 0.3432 - acc: 0.8749 - val_loss: 0.3150 - val_acc: 0.8760\n",
      "\n",
      "Epoch 5/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 10us/step - loss: 0.2736 - acc: 0.8974 - val_loss: 0.2690 - val_acc: 0.8922\n",
      "\n",
      "Epoch 6/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 11us/step - loss: 0.2320 - acc: 0.9132 - val_loss: 0.2399 - val_acc: 0.9061\n",
      "\n",
      "Epoch 7/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 10us/step - loss: 0.2040 - acc: 0.9238 - val_loss: 0.2210 - val_acc: 0.9126\n",
      "\n",
      "Epoch 8/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 10us/step - loss: 0.1834 - acc: 0.9326 - val_loss: 0.2082 - val_acc: 0.9188\n",
      "\n",
      "Epoch 9/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 1s 15us/step - loss: 0.1679 - acc: 0.9383 - val_loss: 0.1981 - val_acc: 0.9204\n",
      "\n",
      "Epoch 10/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 13us/step - loss: 0.1544 - acc: 0.9434 - val_loss: 0.1913 - val_acc: 0.9229] - ETA: 0s - loss: 0.1646 - acc:\n",
      "\n",
      "Epoch 11/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 12us/step - loss: 0.1436 - acc: 0.9477 - val_loss: 0.1870 - val_acc: 0.9245\n",
      "\n",
      "Epoch 12/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 13us/step - loss: 0.1343 - acc: 0.9513 - val_loss: 0.1824 - val_acc: 0.9259\n",
      "\n",
      "Epoch 13/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 13us/step - loss: 0.1263 - acc: 0.9546 - val_loss: 0.1801 - val_acc: 0.9252\n",
      "\n",
      "Epoch 14/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 11us/step - loss: 0.1197 - acc: 0.9572 - val_loss: 0.1785 - val_acc: 0.9269\n",
      "\n",
      "Epoch 15/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 11us/step - loss: 0.1133 - acc: 0.9594 - val_loss: 0.1776 - val_acc: 0.9266\n",
      "\n",
      "Epoch 16/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 13us/step - loss: 0.1080 - acc: 0.9621 - val_loss: 0.1776 - val_acc: 0.9274\n",
      "\n",
      "Epoch 17/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 10us/step - loss: 0.1030 - acc: 0.9642 - val_loss: 0.1780 - val_acc: 0.9275\n",
      "\n",
      "Epoch 18/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 12us/step - loss: 0.0985 - acc: 0.9657 - val_loss: 0.1786 - val_acc: 0.9272\n",
      "\n",
      "Epoch 19/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 1s 15us/step - loss: 0.0945 - acc: 0.9675 - val_loss: 0.1797 - val_acc: 0.9275\n",
      "\n",
      "Epoch 20/20\n",
      "34479/34479 [==============================]34479/34479 [==============================] - 0s 14us/step - loss: 0.0911 - acc: 0.9686 - val_loss: 0.1813 - val_acc: 0.9276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_val,\n",
    "                    y_val,\n",
    "                    epochs=20,\n",
    "                    batch_size=400,\n",
    "                    validation_data=(x_validation, y_validation),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19063/19063 [==============================]19063/19063 [==============================] - 1s 34us/step\n",
      "\n",
      "[0.17635273924499495, 0.93159523684624668]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   6   79    9  378   24   16  135  678 4201   14   30   13 2333    0\n",
      "     0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "quote = tokenize.texts_to_sequences([\"I know you want this for life Taking pictures with all my ice\"])\n",
    "quote = keras.preprocessing.sequence.pad_sequences(quote, value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=20)\n",
    "quote.shape\n",
    "\n",
    "print(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================]1/1 [==============================] - 0s 16ms/step\n",
      "\n",
      "[[ 0.02279586]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(quote,verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('shake_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3033, 39, 2661, 2013, 44, 565, 2733, 9, 3222, 3780, 607, 171, 103, 292, 20, 18, 1, 95, 32]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tokenize2.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
